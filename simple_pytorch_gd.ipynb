{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm\n",
    "\n",
    "참고문헌 <br>\n",
    "[1] https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21 <br>\n",
    "[2] https://stackoverflow.com/questions/71827833/why-pytorch-cant-calculate-gradient-in-the-loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## a quadratic function\n",
    "Let’s take a simple quadratic function defined as:\n",
    "$$ f(x) = x^2-4x+1$$\n",
    "\n",
    "Because it is an univariate function a gradient function is:\n",
    "$$ \\frac{df(x)}{dx}=2x-4$$\n",
    "\n",
    "Let’s write these functions in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    y = x**2 -4*x +1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_desecnt(start: float, lr: float, max_iter: int, tol: float=0.01):\n",
    "    steps = [start]\n",
    "    x=torch.tensor( start , requires_grad=True)\n",
    "    for i in range(max_iter):\n",
    "        x.retain_grad() #꼭 사용해야함.\n",
    "        y = forward(x)\n",
    "        y.backward()\n",
    "        diff = - lr*x.grad\n",
    "        if abs(diff) < tol:\n",
    "            break\n",
    "        x = x - lr*x.grad\n",
    "        steps.append(x.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(9.0 , requires_grad=True)\n",
    "k = x.float()\n",
    "print(k.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n"
     ]
    }
   ],
   "source": [
    "print(x.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.6000, grad_fn=<SubBackward0>)\n",
      "tensor(6.4800, grad_fn=<SubBackward0>)\n",
      "tensor(5.5840, grad_fn=<SubBackward0>)\n",
      "tensor(4.8672, grad_fn=<SubBackward0>)\n",
      "tensor(4.2938, grad_fn=<SubBackward0>)\n",
      "tensor(3.8350, grad_fn=<SubBackward0>)\n",
      "tensor(3.4680, grad_fn=<SubBackward0>)\n",
      "tensor(3.1744, grad_fn=<SubBackward0>)\n",
      "tensor(2.9395, grad_fn=<SubBackward0>)\n",
      "tensor(2.7516, grad_fn=<SubBackward0>)\n",
      "tensor(2.6013, grad_fn=<SubBackward0>)\n",
      "tensor(2.4810, grad_fn=<SubBackward0>)\n",
      "tensor(2.3848, grad_fn=<SubBackward0>)\n",
      "tensor(2.3079, grad_fn=<SubBackward0>)\n",
      "tensor(2.2463, grad_fn=<SubBackward0>)\n",
      "tensor(2.1970, grad_fn=<SubBackward0>)\n",
      "tensor(2.1576, grad_fn=<SubBackward0>)\n",
      "tensor(2.1261, grad_fn=<SubBackward0>)\n",
      "tensor(2.1009, grad_fn=<SubBackward0>)\n",
      "tensor(2.0807, grad_fn=<SubBackward0>)\n",
      "tensor(2.0646, grad_fn=<SubBackward0>)\n",
      "tensor(2.0517, grad_fn=<SubBackward0>)\n",
      "tensor(2.0413, grad_fn=<SubBackward0>)\n",
      "tensor(2.0331, grad_fn=<SubBackward0>)\n",
      "tensor(2.0264, grad_fn=<SubBackward0>)\n",
      "tensor(2.0212, grad_fn=<SubBackward0>)\n",
      "tensor(2.0169, grad_fn=<SubBackward0>)\n",
      "tensor(2.0135, grad_fn=<SubBackward0>)\n",
      "tensor(2.0108, grad_fn=<SubBackward0>)\n",
      "tensor(2.0087, grad_fn=<SubBackward0>)\n",
      "tensor(2.0069, grad_fn=<SubBackward0>)\n",
      "tensor(2.0055, grad_fn=<SubBackward0>)\n",
      "tensor(2.0044, grad_fn=<SubBackward0>)\n",
      "tensor(2.0035, grad_fn=<SubBackward0>)\n",
      "tensor(2.0028, grad_fn=<SubBackward0>)\n",
      "tensor(2.0023, grad_fn=<SubBackward0>)\n",
      "tensor(2.0018, grad_fn=<SubBackward0>)\n",
      "tensor(2.0015, grad_fn=<SubBackward0>)\n",
      "tensor(2.0012, grad_fn=<SubBackward0>)\n",
      "tensor(2.0009, grad_fn=<SubBackward0>)\n",
      "tensor(2.0007, grad_fn=<SubBackward0>)\n",
      "tensor(2.0006, grad_fn=<SubBackward0>)\n",
      "tensor(2.0005, grad_fn=<SubBackward0>)\n",
      "tensor(2.0004, grad_fn=<SubBackward0>)\n",
      "tensor(2.0003, grad_fn=<SubBackward0>)\n",
      "tensor(2.0002, grad_fn=<SubBackward0>)\n",
      "tensor(2.0002, grad_fn=<SubBackward0>)\n",
      "tensor(2.0002, grad_fn=<SubBackward0>)\n",
      "tensor(2.0001, grad_fn=<SubBackward0>)\n",
      "tensor(2.0001, grad_fn=<SubBackward0>)\n",
      "tensor(2.0001, grad_fn=<SubBackward0>)\n",
      "tensor(2.0001, grad_fn=<SubBackward0>)\n",
      "tensor(2.0001, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n",
      "tensor(2.0000, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "start = 9.0\n",
    "x=torch.tensor( start , requires_grad=True)\n",
    "for i in range(100):\n",
    "    x.retain_grad() #꼭 사용해야함.\n",
    "    y = forward(x)\n",
    "    y.backward()\n",
    "    #y.requires_grad = True\n",
    "    #k = y.grad\n",
    "    #print(type(y.grad))\n",
    "    x = x - 0.1*x.grad\n",
    "    print(x.float())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
